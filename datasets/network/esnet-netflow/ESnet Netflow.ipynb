{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESnet Netflow Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python API\n",
    "The python package pynfdump is a front-end to the tool nfdump providing a programmable interface that is very useful when processing netflow data. This package also include remote (via ssh) access.\n",
    "\n",
    "Unfortunately, the version of the package that can be installed via pip is quite old and does not work. The latest checked in version in the github repository works: https://github.com/JustinAzoff/pynfdump/blob/master/pynfdump/nfdump.py\n",
    "\n",
    "Also, while very useful, the pynfdump package does not returns all of the features available in netflow data, in particular, it does not return the \"next hop\" feature.\n",
    "\n",
    "This notebook uses a modified version of nfdump.py that not only returns all of the features, but parses them in easy to use tables. The code can be found at the bottom of this page and needs to be executed before running any of the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following retrieves netflow data from router wash-cr5.\n",
    "\n",
    "Note that paths and remote hosts may be different with your data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remote_root_path='/nfs/netflow/data'\n",
    "remote_host='flowbox1.es.net'\n",
    "router='wash-cr5'\n",
    "path = '/nfs/netflow/data'\n",
    "\n",
    "dumper = Dumper(path,sources=[router],profile='raw',remote_host=remote_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dumper.set_where(start=\"2018-02-20 11:00\", end=\"2018-02-20 15:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "records = dumper.search(limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = import_data(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134.158.84.23 134.79.128.10 134.55.36.45 134.55.36.45\n",
      "160.91.22.86 172.217.15.67 134.55.219.2 134.55.219.2\n",
      "151.101.202.62 134.167.1.1 134.55.42.69 134.55.42.69\n"
     ]
    }
   ],
   "source": [
    "for srcip,dstip,next_hop,next_hop_bgp in zip(results['sa'],results['da'],results['nh'],results['nhb']):\n",
    "    print srcip,dstip,next_hop,next_hop_bgp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'al': ['    0.000', '    0.000', '    0.000'],\n",
       " 'cl': ['    0.000', '    0.000', '    0.000'],\n",
       " 'da': ['134.79.128.10', '172.217.15.67', '134.167.1.1'],\n",
       " 'das': ['3671', '15169', '291'],\n",
       " 'dir': ['0', '0', '0'],\n",
       " 'dmk': ['16', '24', '16'],\n",
       " 'dp': ['36443', '443', '11081'],\n",
       " 'dtos': ['0', '0', '0'],\n",
       " 'dvln': ['0', '0', '0'],\n",
       " 'eng': ['0/0', '0/0', '0/0'],\n",
       " 'exid': ['1', '1', '1'],\n",
       " 'flg': ['.A....', '.AP...', '.A....'],\n",
       " 'fwd': ['64', '64', '64'],\n",
       " 'ibyt': ['60000000', '304000', '1420000'],\n",
       " 'idmc': ['00:00:00:00:00:00', '00:00:00:00:00:00', '00:00:00:00:00:00'],\n",
       " 'in': ['12', '14', '17'],\n",
       " 'ipkt': ['40000', '1000', '1000'],\n",
       " 'ismc': ['00:00:00:00:00:00', '00:00:00:00:00:00', '00:00:00:00:00:00'],\n",
       " 'mpls1': ['0-0-0', '0-0-0', '0-0-0'],\n",
       " 'mpls10': ['0-0-0', '0-0-0', '0-0-0'],\n",
       " 'mpls2': ['0-0-0', '0-0-0', '0-0-0'],\n",
       " 'mpls3': ['0-0-0', '0-0-0', '0-0-0'],\n",
       " 'mpls4': ['0-0-0', '0-0-0', '0-0-0'],\n",
       " 'mpls5': ['0-0-0', '0-0-0', '0-0-0'],\n",
       " 'mpls6': ['0-0-0', '0-0-0', '0-0-0'],\n",
       " 'mpls7': ['0-0-0', '0-0-0', '0-0-0'],\n",
       " 'mpls8': ['0-0-0', '0-0-0', '0-0-0'],\n",
       " 'mpls9': ['0-0-0', '0-0-0', '0-0-0'],\n",
       " 'nh': ['134.55.36.45', '134.55.219.2', '134.55.42.69'],\n",
       " 'nhb': ['134.55.36.45', '134.55.219.2', '134.55.42.69'],\n",
       " 'obyt': ['0', '0', '0'],\n",
       " 'odmc': ['00:00:00:00:00:00', '00:00:00:00:00:00', '00:00:00:00:00:00'],\n",
       " 'opkt': ['0', '0', '0'],\n",
       " 'osmc': ['00:00:00:00:00:00', '00:00:00:00:00:00', '00:00:00:00:00:00'],\n",
       " 'out': ['16', '17', '14'],\n",
       " 'pr': ['TCP', 'TCP', 'TCP'],\n",
       " 'ra': ['134.55.200.147', '134.55.200.147', '134.55.200.147'],\n",
       " 'sa': ['134.158.84.23', '160.91.22.86', '151.101.202.62'],\n",
       " 'sas': ['789', '50', '54113'],\n",
       " 'sl': ['    0.000', '    0.000', '    0.000'],\n",
       " 'smk': ['16', '16', '22'],\n",
       " 'sp': ['53400', '64130', '443'],\n",
       " 'stos': ['32', '0', '0'],\n",
       " 'svln': ['0', '0', '0'],\n",
       " 'td': ['59.370', '0.000', '0.000'],\n",
       " 'te': ['2018-02-20 10:59:59', '2018-02-20 10:59:49', '2018-02-20 10:59:49'],\n",
       " 'tr': ['2018-02-20 11:00:00.086\\n',\n",
       "  '2018-02-20 11:00:00.086\\n',\n",
       "  '2018-02-20 11:00:00.086\\n'],\n",
       " 'ts': ['2018-02-20 10:59:00', '2018-02-20 10:59:49', '2018-02-20 10:59:49']}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = !ssh flowbox1.es.net  'nfdump -M /nfs/netflow/data/raw/wash-cr5/2018/02/20 -R . -c 3 -o csv -t 2018/02/20.00:00-2018/02/20.15:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Src IP Addr     Dst IP Addr\n",
      "  198.124.238.49 198.129.254.138\n",
      "Summary: total flows: 1, total bytes: 42000, total packets: 1000, avg bps: 0, avg pps: 0, avg bpp: 0\n",
      "Time window: 2018-02-19 23:58:59 - 2018-02-20 00:04:58\n",
      "Total flows processed: 7613, Blocks skipped: 0, Bytes read: 1048476\n",
      "Sys: 0.071s flows/second: 105753.7   Wall: 0.009s flows/second: 838898.1  \n",
      "Killed by signal 1.\n"
     ]
    }
   ],
   "source": [
    "!ssh flowbox1.es.net  'nfdump -M /nfs/netflow/data/raw/wash-cr5/2018/02/20 -R . -c 1 -o fmt:%sa%da -t 2018/02/20.00:00-2018/02/20.15:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('/tmp/mbps.csv') as f:\n",
    "    for row in csv.reader(f):\n",
    "        res['mbps'].append(float(row[1].split(':')[1].lstrip()))\n",
    "        res['duration'].append(row[2].split(':')[1][:-4].lstrip())\n",
    "        res['size'].append(row[0].split(':')[-1].lstrip())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "r = csv.reader([z[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018-02-20 00:00:00', '2018-02-20 00:00:00', '0.000', '198.124.238.49', '198.129.254.138', '8807', '9641', 'UDP', '......', '64', '0', '1000', '42000', '0', '0', '12', '16', '65291', '292', '30', '30', '0', '0', '134.55.36.45', '134.55.36.45', '0', '0', '00:00:00:00:00:00', '00:00:00:00:00:00', '00:00:00:00:00:00', '00:00:00:00:00:00', '0-0-0', '0-0-0', '0-0-0', '0-0-0', '0-0-0', '0-0-0', '0-0-0', '0-0-0', '0-0-0', '0-0-0', '    0.000', '    0.000', '    0.000', '134.55.200.147', '0/0', '1', '2018-02-20 00:00:10.187']\n"
     ]
    }
   ],
   "source": [
    "for x in r:\n",
    "    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ts,te,td,sa,da,sp,dp,pr,flg,fwd,stos,ipkt,ibyt,opkt,obyt,in,out,sas,das,smk,dmk,dtos,dir,nh,nhb,svln,dvln,ismc,odmc,idmc,osmc,mpls1,mpls2,mpls3,mpls4,mpls5,mpls6,mpls7,mpls8,mpls9,mpls10,cl,sl,al,ra,eng,exid,tr',\n",
       " '2018-02-20 00:00:00,2018-02-20 00:00:00,0.000,198.124.238.49,198.129.254.138,8807,9641,UDP,......,64,0,1000,42000,0,0,12,16,65291,292,30,30,0,0,134.55.36.45,134.55.36.45,0,0,00:00:00:00:00:00,00:00:00:00:00:00,00:00:00:00:00:00,00:00:00:00:00:00,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,    0.000,    0.000,    0.000,134.55.200.147,0/0,1,2018-02-20 00:00:10.187',\n",
       " '2018-02-20 00:00:00,2018-02-20 00:00:00,0.000,130.199.39.193,96.127.69.64,54805,443,TCP,.A....,64,0,1000,1500000,0,0,12,16,43,8987,16,17,0,0,134.55.36.45,134.55.36.45,0,0,00:00:00:00:00:00,00:00:00:00:00:00,00:00:00:00:00:00,00:00:00:00:00:00,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,    0.000,    0.000,    0.000,134.55.200.147,0/0,1,2018-02-20 00:00:10.187',\n",
       " '2018-02-20 00:00:00,2018-02-20 00:00:00,0.000,131.225.188.104,188.184.83.197,14038,9636,TCP,.AP...,64,0,1000,65000,0,0,16,12,3152,513,16,16,0,0,134.55.36.33,134.55.36.33,0,0,00:00:00:00:00:00,00:00:00:00:00:00,00:00:00:00:00:00,00:00:00:00:00:00,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,0-0-0,    0.000,    0.000,    0.000,134.55.200.147,0/0,1,2018-02-20 00:00:10.187',\n",
       " 'Summary',\n",
       " 'flows,bytes,packets,avg_bps,avg_pps,avg_bpp',\n",
       " '3,1607000,3000,0,0,0',\n",
       " 'Killed by signal 1.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annex: modified nfdump.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nfdump.py\n",
    "# Copyright (C) 2008 Justin Azoff JAzoff@uamail.albany.edu\n",
    "#\n",
    "# This module is released under the MIT License:\n",
    "# http://www.opensource.org/licenses/mit-license.php\n",
    "\"\"\"\n",
    "Python frontend to the nfdump CLI\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dateutil.parser import parse as parse_date\n",
    "import datetime\n",
    "fromtimestamp = datetime.datetime.fromtimestamp\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "import select\n",
    "import commands\n",
    "\n",
    "from IPy import IP\n",
    "\n",
    "FILE_FMT = \"%Y %m %d %H %M\".replace(\" \",\"\")\n",
    "\n",
    "def load_protocols():\n",
    "    #2.4 doesn't have socket.getprotocol by id\n",
    "    f = open(\"/etc/protocols\")\n",
    "    protocols = {}\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            break\n",
    "    for line in f:\n",
    "        if not line.strip(): break\n",
    "        if line.startswith(\"#\"): continue\n",
    "        proto, num,_ = line.split(None,2)\n",
    "        protocols[int(num)] = proto\n",
    "    protocols[0]='ip'\n",
    "    f.close()\n",
    "    return protocols\n",
    "\n",
    "def date_to_fn(date):\n",
    "    return 'nfcapd.' + date.strftime(FILE_FMT)\n",
    "\n",
    "STDOUT = 1\n",
    "STDERR = 2\n",
    "def mycommunicate(cmds):\n",
    "    pipe = Popen(cmds, stdout=PIPE,stderr=PIPE)\n",
    "    read_set = [pipe.stderr, pipe.stdout]\n",
    "\n",
    "    waited = False\n",
    "\n",
    "    #from the subprocess module\n",
    "    try :\n",
    "        while read_set:\n",
    "            rlist, wlist, xlist = select.select(read_set, [], [])\n",
    "\n",
    "            if pipe.stdout in rlist:\n",
    "                data = pipe.stdout.readline()\n",
    "                if data == \"\":\n",
    "                    pipe.stdout.close()\n",
    "                    read_set.remove(pipe.stdout)\n",
    "                else:\n",
    "                    yield STDOUT, data\n",
    "\n",
    "            if pipe.stderr in rlist:\n",
    "                data = os.read(pipe.stderr.fileno(), 1024)\n",
    "                if data == \"\" or data.startswith(\"Killed by signal 1.\"):\n",
    "                    pipe.stderr.close()\n",
    "                    read_set.remove(pipe.stderr)\n",
    "                else:\n",
    "                    yield STDERR, data\n",
    "    #work around python2.4 issue:\n",
    "    #SyntaxError: 'yield' not allowed in a 'try' block with a 'finally' clause \n",
    "    except:\n",
    "        pipe.wait()\n",
    "        waited = True\n",
    "    if not waited:\n",
    "        pipe.wait()\n",
    "\n",
    "def run(cmds):\n",
    "    #print (cmds)\n",
    "    for fd, data in mycommunicate(cmds):\n",
    "        if fd == STDERR:\n",
    "            raise NFDumpError(data)\n",
    "        else:\n",
    "            yield data\n",
    "\n",
    "def maybe_int(val):\n",
    "    try:\n",
    "        val = int(val)\n",
    "    except TypeError:\n",
    "        pass\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return val\n",
    "\n",
    "def maybe_split(val, sep):\n",
    "    if hasattr(val, 'split'):\n",
    "        return val.split(sep)\n",
    "    return val\n",
    "\n",
    "def flags_to_str(flags):\n",
    "    s = \"\"\n",
    "    s += flags & 32 and 'U' or '.'\n",
    "    s += flags & 16 and 'A' or '.'\n",
    "    s += flags &  8 and 'P' or '.'\n",
    "    s += flags &  4 and 'R' or '.'\n",
    "    s += flags &  2 and 'S' or '.'\n",
    "    s += flags &  1 and 'F' or '.'\n",
    "    return s\n",
    "\n",
    "def import_data(data):\n",
    "    results = {}\n",
    "    features = 'ts,te,td,sa,da,sp,dp,pr,flg,fwd,stos,ipkt,ibyt,opkt,obyt,in,out,sas,das,smk,dmk,dtos,dir,nh,nhb,svln,dvln,ismc,odmc,idmc,osmc,mpls1,mpls2,mpls3,mpls4,mpls5,mpls6,mpls7,mpls8,mpls9,mpls10,cl,sl,al,ra,eng,exid,tr'\n",
    "    features = features.split(',')\n",
    "    for feature in features:\n",
    "        results[feature] = []\n",
    "    for entry in data:\n",
    "        for index in range(len(features)):\n",
    "            results[features[index]].append(entry[index])\n",
    "    return results\n",
    "\n",
    "class NFDumpError(Exception):\n",
    "    pass\n",
    "\n",
    "class Dumper:\n",
    "    def __init__(self, datadir='/', profile='live',sources=None,remote_host=None,executable_path='nfdump'):\n",
    "        if not datadir.endswith(\"/\"):\n",
    "            datadir = datadir + '/'\n",
    "        self.datadir = datadir\n",
    "        self.profile = profile\n",
    "        self.sources = maybe_split(sources, ',')\n",
    "        self.remote_host = remote_host\n",
    "        self.exec_path = executable_path\n",
    "        if os.path.isdir(self.exec_path):\n",
    "            self.exec_path = os.path.join(self.exec_path, \"nfdump\")\n",
    "        self.set_where()\n",
    "        self.protocols = load_protocols()\n",
    "\n",
    "    def set_where(self, start=None, end=None, filename=None,dirfiles=None, stdin=False):\n",
    "        \"\"\"Set the timeframe of the nfdump query.\n",
    "        Specify one of the following:\n",
    "            * The start date\n",
    "            * The start and end date\n",
    "            * one of the filename,dirfiles, or stdin options\n",
    "        :param start: Start date and time\n",
    "        :param end: Start date and time\n",
    "        :param filename: Search this single filename\n",
    "        :param dirfiles: Search this directory\n",
    "        :param stdin:    Search stdin\n",
    "        \"\"\"\n",
    "        \n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        self.sd = self.ed = None\n",
    "\n",
    "        if start:\n",
    "            self.sd = parse_date(start)\n",
    "        if end:\n",
    "            self.ed = parse_date(end)\n",
    "\n",
    "        if not self.sd:\n",
    "            self._where = '.'\n",
    "        else:\n",
    "            self._where = date_to_fn(self.sd)\n",
    "            if self.ed:\n",
    "                self._where += \":\" + date_to_fn(self.ed)\n",
    "\n",
    "        if dirfiles:\n",
    "            self._where = dirfiles\n",
    "\n",
    "        self.filename = filename\n",
    "        if stdin:\n",
    "            self.filename = '-'\n",
    "\n",
    "    def _arg_escape(self, arg):\n",
    "        \"\"\"Escape any arguments so that they can be passed over SSH\"\"\"\n",
    "        if self.remote_host:\n",
    "            return commands.mkarg(arg)\n",
    "        else:\n",
    "            return arg\n",
    "\n",
    "    def _base_cmd(self):\n",
    "        cmd = []\n",
    "        if self.remote_host:\n",
    "            cmd = ['ssh', self.remote_host]\n",
    "        cmd.extend([self.exec_path, '-q', '-o', 'csv'])\n",
    "\n",
    "        if self.datadir and self.sources and self.profile:\n",
    "            sources = ':'.join(self.sources)\n",
    "            d = os.path.join(self.datadir, self.profile, sources)\n",
    "            cmd.extend(['-M', d])\n",
    "\n",
    "        if self.filename:\n",
    "            cmd.extend(['-r', self.filename])\n",
    "        else:\n",
    "            cmd.extend(['-R', self._where])\n",
    "        return cmd\n",
    "\n",
    "    def search(self, query='', filterfile=None, aggregate=None, statistics=None, statistics_order=None,limit=None):\n",
    "        \"\"\"Run nfdump with the following arguments\n",
    "        :param query: The nfdump filter\n",
    "        :param filterfile: an optional file containing a nfdump filter\n",
    "        :param aggregate: (True OR comma sep string OR list) of\n",
    "            * srcip     - Source IP Address\n",
    "            * dstip     - Destination IP Address\n",
    "            * srcport   - Source Port\n",
    "            * dstport   - Destination Port\n",
    "        :param statistics: Generate netflow statistics info, one of\n",
    "            * srcip     - Source IP Address\n",
    "            * dstip     - Destination IP Address\n",
    "            * ip        - Any IP Address\n",
    "            * srcport   - Source Port\n",
    "            * dstport   - Destination Port\n",
    "            * port      - Any Port\n",
    "            * srcas     - Source ASN\n",
    "            * dstas     - Destination ASN\n",
    "            * as        - Any ASN\n",
    "            * inif      - Incoming Interface\n",
    "            * outif     - Outgoing Interface\n",
    "            * proto     - Protocol\n",
    "        :param statistics_order: one of\n",
    "            * packets\n",
    "            * bytes\n",
    "            * flows\n",
    "            * bps       - Bytes Per Second\n",
    "            * pps       - Packers Per Second\n",
    "            * bpp.      - Bytes Per Packet\n",
    "        :param limit: number of results\n",
    "        \"\"\"\n",
    "\n",
    "        cmd = self._base_cmd()\n",
    "\n",
    "        if aggregate and statistics:\n",
    "            raise NFDumpError(\"Specify only one of aggregate and statistics\")\n",
    "\n",
    "        if statistics:\n",
    "            s_arg = statistics\n",
    "            if statistics_order:\n",
    "                s_arg = \"%s/%s\" % (statistics, statistics_order)\n",
    "\n",
    "            cmd.extend([\"-s\", s_arg])\n",
    "\n",
    "        if aggregate:\n",
    "            if aggregate is True:\n",
    "                cmd.append(\"-a\")\n",
    "            else:\n",
    "                aggregate = maybe_split(aggregate, ',')\n",
    "                aggregate = ','.join(aggregate)\n",
    "                aggregate = aggregate.replace(\" \",\"\")\n",
    "                cmd.extend([\"-a\", \"-A\", self._arg_escape(aggregate)])\n",
    "\n",
    "        if limit:\n",
    "            if statistics:\n",
    "                cmd.extend(['-n',str(limit)])\n",
    "            else:\n",
    "                cmd.extend(['-c',str(limit)])\n",
    "\n",
    "        if filterfile:\n",
    "            cmd.extend(['-f', filterfile])\n",
    "        else:\n",
    "            cmd.append(self._arg_escape(query))\n",
    "\n",
    "        out = run(cmd)\n",
    "        if statistics:\n",
    "            return self.parse_stats(out, object_field=statistics)\n",
    "        else:\n",
    "            return self.parse_search(out)\n",
    "\n",
    "    def parse_search(self, out):\n",
    "        #    snprintf(data_string, STRINGSIZE-1 ,\"%i|%u|%u|%u|%u|%u|%u|%u|%u|%u|%u|%u|%u|%u|%u|%u|%u|%u|%u|%u|%u|%u|%llu|%llu\",\n",
    "        #                0 af, 1 r->first, 2 r->msec_first ,3 r->last, 4 r->msec_last, 5 r->prot,\n",
    "        #                6 sa[0], 7 sa[1], 8 sa[2], 9 sa[3], 10 r->srcport, 11 da[0], 12 da[1], 13 da[2], 14 da[3], 15 r->dstport,\n",
    "        #                16 r->srcas, 17 r->dstas, 18 r->input, 19 r->output,\n",
    "        #                20 r->tcp_flags, 21 r->tos, 22 (unsigned long long)r->dPkts, 23 (unsigned long long)r->dOctets);\n",
    "\n",
    "        for line in out:\n",
    "            row = line.split(',')\n",
    "            yield row\n",
    "\n",
    "    def parse_stats(self, out,object_field):\n",
    "        for line in out:\n",
    "            parts = line.split(\"|\")\n",
    "            parts = [maybe_int(x) for x in parts]\n",
    "            if not len(parts) > 10:\n",
    "                #print line\n",
    "                continue\n",
    "            if '0|0|0|0' in line:\n",
    "                object_idx = 9\n",
    "            else:\n",
    "                object_idx = 6\n",
    "            row = {\n",
    "                'af':           parts[0],\n",
    "                'first':        fromtimestamp(parts[1]),\n",
    "                #'msec_first':   parts[2],\n",
    "                'last':         fromtimestamp(parts[3]),\n",
    "                #'msec_last':    parts[4],\n",
    "                'prot':         self.protocols.get(parts[5], parts[5]),\n",
    "                object_field:   parts[object_idx],\n",
    "                'flows':        parts[object_idx+1],\n",
    "                'packets':      parts[object_idx+2],\n",
    "                'bytes':        parts[object_idx+3],\n",
    "                'pps':          parts[object_idx+4],\n",
    "                'bps':          parts[object_idx+5],\n",
    "                'bpp':          parts[object_idx+6],\n",
    "            }\n",
    "\n",
    "            if 'ip' in object_field:\n",
    "                row[object_field] = IP(row[object_field])\n",
    "\n",
    "            yield row\n",
    "\n",
    "\n",
    "    def list_profiles(self):\n",
    "        \"\"\"Return a list of the nfsen profiles\"\"\"\n",
    "        if not self.remote_host:\n",
    "            return os.listdir(self.datadir)\n",
    "        else:\n",
    "            return run(['ssh', self.remote_host, '/bin/ls', self.datadir])[0].split()\n",
    "\n",
    "    def get_profile_data(self, profile=None):\n",
    "        \"\"\"Return a dictionary of the nfsen profile data\"\"\"\n",
    "        p = profile or self.profile\n",
    "    \n",
    "        path = os.path.join(self.datadir,p,'profile.dat')\n",
    "    \n",
    "        if not self.remote_host:\n",
    "            data = open(path).read()\n",
    "        else:\n",
    "            data = run(['ssh', self.remote_host, '/bin/cat', path])[0]\n",
    "\n",
    "        ret = {}\n",
    "        sourcelist = []\n",
    "        for line in data.splitlines():\n",
    "            if not line: continue\n",
    "            if line[0] in ' #': continue\n",
    "            key, val = line.split(\" = \", 1)\n",
    "            if key == 'channel':\n",
    "                chan = val.split(\":\")[0]\n",
    "                sourcelist.append(chan)\n",
    "                continue\n",
    "\n",
    "            ret[key] = maybe_int(val)\n",
    "        if sourcelist:\n",
    "            ret['sourcelist'] = sourcelist\n",
    "        return ret\n",
    "\n",
    "    def flow_stats(self):\n",
    "        \"\"\"Run nfdump -I to get flow stats\"\"\"\n",
    "        cmd = self._base_cmd()\n",
    "        cmd.append(\"-I\")\n",
    "        out = run(cmd)\n",
    "        return self.parse_flow_stats(out)\n",
    "\n",
    "    def parse_flow_stats(self, out):\n",
    "        stats = {}\n",
    "        for line in out:\n",
    "            key, value = line.split(\": \")\n",
    "            key = key.strip().lower()\n",
    "            value = maybe_int(value.strip())\n",
    "            stats[key] = value\n",
    "        return stats\n",
    "\n",
    "def search_file(filename, query='', filterfile=None, aggregate=None, statistics=None, statistics_order=None,limit=None):\n",
    "    \"\"\"Search a single nfcapd file\n",
    "    :param filename: the file to search\n",
    "    The rest of the options are passed directly to :func:`Dumper.search`\n",
    "    \"\"\"\n",
    "\n",
    "    d = Dumper()\n",
    "    d.set_where(filename=filename)\n",
    "    return d.search(query, filterfile, aggregate, statistics, statistics_order, limit)\n",
    "\n",
    "def flow_stats_file(filename):\n",
    "    \"\"\"Get flow stats for a single nfcapd file\"\"\"\n",
    "    d = Dumper()\n",
    "    d.set_where(filename=filename)\n",
    "    return d.flow_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
