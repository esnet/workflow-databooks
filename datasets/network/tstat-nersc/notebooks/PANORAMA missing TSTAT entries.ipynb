{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing TSTAT data on gridFTP/Globus transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reported problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Panorama project ran a series of Globlus/gridFTP from the host stash.osgconnect.net to the NERSC DTN cluster composed of a dozen hosts. Two data sets were then used to correlate information about the flows:\n",
    "\n",
    "  . TSTAT data collected on the NERSC hosts and sent to two ELK clusters, NERSC cluster. The Panorama cluster is used by the Panorama team and the NERSC cluster is used when developing this notebook.\n",
    "  . Globus logs collected on stash.osgconnect.net. This notebook was developed after receiving a subset of this data set which contains user information and therefore, while being used in this notebook, it cannot be made public. \n",
    "  \n",
    "Having access to both data sets is required in order to execute this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation summary and status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The investigation followed the following steps:\n",
    "\n",
    ". Repeat the reported problem on the NERSC cluster. Problem was replicated, therefore, there are no specific issues related to the Panorama cluster. . Problem was reported using the ELK Kabana web portal. Repeated problem directly using the ELK REST API. Problem was replicated, so no Kabana related problem. . Recovered TSTAT logs directly from NERSC DTN. Verified that the missing TSTAT were present (existed) on the DTN's.\n",
    "\n",
    "This leaves with the following possible source of the problem:\n",
    "\n",
    ". the tstat-send agent running on the DTN drop data when parsing the TSTAT logs. . the tstat-send agend and/or the NERSC RabbitMQ exchanger drop or stop some of the messages sent by the tstat-send agents. . the ELK clusters drop/ignore the missing entries. That would happen on both cluster (likelyhood ?)\n",
    "\n",
    "Normally, the most likely place to look at would be the tstat-agent. For instance, perhaps the agent misses entries that are received while it processes the log ?\n",
    "\n",
    "However, it was observed on both clusters, using the web portal to query the database, that the systems warn of failing to read some shards. After talking to the people running and supporting the NERSC ELK cluster, it appears there are currently too many indexes in the TSTAT set (about 30 indexes per month). It is not sure yet that this is the cause of missing data and this is still being investigated. This problem would, of course, exist on both clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for debuging this problem. It contains code that queries the ELK clusters, read the Globus logs, DTN logs, parses and pre-process data. It also includes code that correlate datasets and find the missing entries. This code and the comments that comes along is the reasoning that leads to the previous summary and status of the investigation.\n",
    "\n",
    "Note that some of this code is using library available at https://github.com/esnet/workflow-databooks/blob/master/datasets/tstat-nersc/tstat-dtn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the NERSC Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proxy host:········\n",
      "password: ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py:112: UserWarning: Connecting to cheshire.nersc.gov using SSL with verify_certs=False is insecure.\n",
      "  'Connecting to %s using SSL with verify_certs=False is insecure.' % host)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from tstat_nersc import create_tunnel, kill_tunnel, esdb_connect\n",
    "\n",
    "# Create SSH tunnel\n",
    "tunnel = create_tunnel()\n",
    "# Connect to ELK\n",
    "esdb = esdb_connect()\n",
    "\n",
    "# Globals\n",
    "tstat_index = \"dtn-tstat-2018*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve all records of transfers from stash.osgconnect.net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total results:  301\n"
     ]
    }
   ],
   "source": [
    "query={\n",
    "    \"query\":{\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\"match\": { \"source\": \"stash.osgconnect.net\" }}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"sort\": [\n",
    "        { \"start\": \"asc\" }\n",
    "    ]\n",
    "}\n",
    "\n",
    "results = esdb.search(index=tstat_index, body=query, size=500)\n",
    "print \"total results: \", results['hits']['total']\n",
    "raw = results['hits']['hits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters sensor_id (destination DTN at NERSC) and time. Localize time to PST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from pytz import timezone\n",
    "import pytz\n",
    "import datetime\n",
    "\n",
    "def process_raw(raw):\n",
    "    res={} \n",
    "    pst = pytz.timezone('America/Los_Angeles')\n",
    "    for row in raw:\n",
    "        dtn = row['_source']['meta']['sensor_id']\n",
    "        if not dtn in res:\n",
    "            res[dtn] = {'start':[],'end':[],'size':[]}\n",
    "        start = datetime.datetime.fromtimestamp(int(row['_source']['start']))\n",
    "        pst_start = pst.localize(start, is_dst=None)\n",
    "        pst_start.astimezone (pst)\n",
    "        res[dtn]['start'].append(pst_start)\n",
    "        end = datetime.datetime.fromtimestamp(int(row['_source']['end']))\n",
    "        pst_end = pst.localize(end, is_dst=None)\n",
    "        pst_end.astimezone (pst)\n",
    "        res[dtn]['end'].append(pst_end)\n",
    "        res[dtn]['size'].append(int(row['_source']['file_size_MB']))\n",
    "    return res \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tstat = process_raw(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre-process Globus log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import gridftp logs. Only retrieve relevant information.\n",
    "\n",
    "Process data, localize time to GMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ~/Downloads/transfer_logs; grep dtn nersc_gridftp.log  > /tmp/globus.csv\n",
    "!cd ~/Downloads/transfer_logs/globus_logs; grep mbps */* > /tmp/mbps.csv\n",
    "import csv\n",
    "from pytz import timezone\n",
    "import pytz\n",
    "\n",
    "def read_extra():\n",
    "    res = {'mbps':[],'size':[],'duration':[]}\n",
    "    with open('/tmp/mbps.csv') as f:\n",
    "        for row in csv.reader(f):\n",
    "            res['mbps'].append(float(row[1].split(':')[1].lstrip()))\n",
    "            res['duration'].append(row[2].split(':')[1][:-4].lstrip())\n",
    "            res['size'].append(row[0].split(':')[-1].lstrip())\n",
    "    return res\n",
    "\n",
    "def read_globus():\n",
    "    gmt = pytz.timezone('GMT')\n",
    "    with open('/tmp/globus.csv') as f:\n",
    "        res = {'time':[],'dtn':[],'streams':[]}\n",
    "        for row in csv.reader(f):\n",
    "            #t = datetime.datetime.strptime(row[3].split('.')[0], '%Y/%m/%d %H:%M:%S')\n",
    "            t = datetime.datetime.strptime(row[10].split('.')[0], '%Y/%m/%d %H:%M:%S')\n",
    "            gmt_t = gmt.localize(t, is_dst=None)\n",
    "            gmt_t.astimezone (gmt)\n",
    "            res['time'].append(gmt_t)\n",
    "            res['dtn'].append(row[6])\n",
    "            res['streams'].append(row[11])\n",
    "        extra = read_extra()\n",
    "        res.update(extra)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "globus = read_globus() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate both data sets to find missing TSTAT data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detects streams that are in gridftp logs but not in tstat. The correlation is done by matching flows based on starting time. Since the TSTAT data is already filtered (by query) to flows coming from stash.osgconnect.net, we do not have to worry about other flows. A configurable \"time proximity\" in seconds can be set to set the time window for which we can consider that the TSTAT data relates to a given file transfer. The configurable is the optional \"lim\" option provided to check_tstat_transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_tstat_transfer(tstat,dtn,transfer_time,lim=2):\n",
    "    count = 0\n",
    "    for t in tstat[dtn]['start']:\n",
    "        if abs(transfer_time - t).seconds < lim:\n",
    "            count += 1  \n",
    "    return count\n",
    "    \n",
    "def check_transfers(globus, tstat):\n",
    "    incorrect = {'time':[],'dtn':[],'shouldbe':[],'was':[],'mbps':[],'miss':[]}\n",
    "    correct = {'time':[],'dtn':[],'shouldbe':[],'was':[],'mbps':[],'miss':[]}\n",
    "    for t,dtn,mbps,streams in zip(globus['time'],globus['dtn'],globus['mbps'],globus['streams']):\n",
    "        streams = int(streams)\n",
    "        count = 0\n",
    "        if dtn in tstat:\n",
    "            count = check_tstat_transfer(tstat=tstat, dtn=dtn, transfer_time=t)\n",
    "        res = None\n",
    "        if count != streams:\n",
    "            res = incorrect\n",
    "        else:\n",
    "            res = correct\n",
    "        res['time'].append(t)\n",
    "        res['dtn'].append(dtn)\n",
    "        res['shouldbe'].append(streams)\n",
    "        res['was'].append(count)\n",
    "        res['mbps'].append(mbps)\n",
    "        res['miss'].append(streams - count)\n",
    "    return correct,incorrect\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing streams:  25\n"
     ]
    }
   ],
   "source": [
    "found,missing = check_transfers(globus=globus, tstat=tstat)\n",
    "print \"Number of missing streams: \", len(missing['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtn01.nersc.gov 2018-02-28 08:30:12+00:00 309.98 4\n",
      "dtn05.nersc.gov 2018-02-28 09:30:11+00:00 1589.78 4\n",
      "dtn04.nersc.gov 2018-02-28 10:30:11+00:00 1274.91 4\n",
      "dtn07.nersc.gov 2018-02-28 10:30:12+00:00 1965.45 4\n",
      "dtn10.nersc.gov 2018-02-28 11:30:13+00:00 1799.97 4\n",
      "dtn05.nersc.gov 2018-02-28 12:30:11+00:00 689.08 4\n",
      "dtn02.nersc.gov 2018-02-28 12:30:12+00:00 1877.25 4\n",
      "dtn05.nersc.gov 2018-02-28 13:00:12+00:00 1456.44 4\n",
      "dtn05.nersc.gov 2018-02-28 13:30:12+00:00 1422.13 4\n",
      "dtn05.nersc.gov 2018-02-28 14:00:11+00:00 1982.96 4\n",
      "dtn04.nersc.gov 2018-02-28 14:30:14+00:00 1255.23 4\n",
      "dtn09.nersc.gov 2018-02-28 15:30:13+00:00 1388.34 4\n",
      "dtn01.nersc.gov 2018-02-28 15:30:14+00:00 1138.04 4\n",
      "dtn09.nersc.gov 2018-02-28 16:30:11+00:00 1469.77 4\n",
      "dtn04.nersc.gov 2018-02-28 17:30:10+00:00 1778.05 4\n",
      "dtn05.nersc.gov 2018-02-28 18:00:12+00:00 1518.45 4\n",
      "dtn08.nersc.gov 2018-02-28 18:30:13+00:00 1553.42 4\n",
      "dtn10.nersc.gov 2018-02-28 18:30:12+00:00 1482.16 4\n",
      "dtn01.nersc.gov 2018-02-28 19:00:11+00:00 1874.68 1\n",
      "dtn04.nersc.gov 2018-02-28 19:30:12+00:00 1246.44 4\n",
      "dtn01.nersc.gov 2018-02-28 19:30:11+00:00 2027.8 4\n",
      "dtn08.nersc.gov 2018-02-28 20:30:11+00:00 1259.27 4\n",
      "dtn06.nersc.gov 2018-02-28 20:30:11+00:00 551.36 3\n",
      "dtn01.nersc.gov 2018-02-28 21:00:14+00:00 507.42 1\n",
      "dtn01.nersc.gov 2018-02-28 21:30:11+00:00 1548.39 4\n"
     ]
    }
   ],
   "source": [
    "for dtn,t,mbps,miss in zip(missing['dtn'],missing['time'],missing['mbps'],missing['miss']):\n",
    "    print dtn,t,mbps,miss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this result, we see we miss quite a few TSTAT entries. This confirms that the reported problem is indeed real and that there this is not a problem that happens only on the Panorama cluster since this was reproduced using the NERSC cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at patterns in missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very slow flows may be filtered by the tstat-agent running on the host. The following code looks at the missing TSTAT entries, and, after retrieve flow performance data from the Globus data, displays it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtn06.nersc.gov 2018-02-28 08:00:13+00:00 737.89 0\n",
      "dtn06.nersc.gov 2018-02-28 09:00:11+00:00 1985.38 0\n",
      "dtn09.nersc.gov 2018-02-28 09:00:12+00:00 1967.53 0\n",
      "dtn06.nersc.gov 2018-02-28 09:30:12+00:00 1951.92 0\n",
      "dtn08.nersc.gov 2018-02-28 10:00:11+00:00 816.95 0\n",
      "dtn09.nersc.gov 2018-02-28 10:00:12+00:00 1306.28 0\n",
      "dtn10.nersc.gov 2018-02-28 11:00:12+00:00 1613.12 0\n",
      "dtn09.nersc.gov 2018-02-28 12:00:11+00:00 1545.36 0\n",
      "dtn09.nersc.gov 2018-02-28 13:00:13+00:00 2003.69 0\n",
      "dtn06.nersc.gov 2018-02-28 13:30:11+00:00 2009.21 0\n",
      "dtn10.nersc.gov 2018-02-28 14:00:12+00:00 2033.28 0\n",
      "dtn10.nersc.gov 2018-02-28 15:00:11+00:00 861.07 0\n",
      "dtn10.nersc.gov 2018-02-28 16:00:11+00:00 1436.49 0\n",
      "dtn06.nersc.gov 2018-02-28 16:00:12+00:00 1757.23 0\n",
      "dtn06.nersc.gov 2018-02-28 16:30:12+00:00 1430.46 0\n",
      "dtn01.nersc.gov 2018-02-28 17:00:13+00:00 1458.64 0\n",
      "dtn07.nersc.gov 2018-02-28 17:00:14+00:00 1656.85 0\n",
      "dtn06.nersc.gov 2018-02-28 17:30:11+00:00 1861.57 0\n",
      "dtn10.nersc.gov 2018-02-28 18:00:12+00:00 2001.21 0\n",
      "dtn06.nersc.gov 2018-02-28 19:00:12+00:00 1838.63 0\n",
      "dtn08.nersc.gov 2018-02-28 20:00:13+00:00 1632.03 0\n",
      "dtn06.nersc.gov 2018-02-28 20:00:12+00:00 1185.84 0\n",
      "dtn02.nersc.gov 2018-02-28 21:00:14+00:00 1746.3 0\n"
     ]
    }
   ],
   "source": [
    "for dtn,t,mbps,miss in zip(found['dtn'],found['time'],found['mbps'],found['miss']):\n",
    "    print dtn,t,mbps,miss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the \"missing\" flows seem to have exceeded 400 Mbps. Since all flows were made of 4 different TCP streams, we can deduct that some of the missing flows were above 100 Mbps. In other words, we cannot just blames flows for being too slow for the configured filter.\n",
    "\n",
    "Some other patterms is under investigation. For instance, flows were initiated every 30 minutes starting at the top of the hour. It is possible that the cron job running in the DTN hosts may ran the same time perhaps running into a bug in the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking TSTAT logs on NERSC DTN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to verify if the TSTAT collection capture the missing flows. The NERSC DTN have been keeping all of its TSTAT measurement locally. After uploading the logs from dtn01.nersc.gov for the period of time when the problem was detecting, the following code parses the log and pre-processes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pytz import timezone\n",
    "import pytz\n",
    "\n",
    "def read_tstat_from_dtn(filename):\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        pst = pytz.timezone('America/Los_Angeles')\n",
    "        res = {'start':[],'end':[],'duration':[],'size':[],'speed':[]}\n",
    "        for row in f.readlines():\n",
    "            row = row.split(':')[1].split(' ')\n",
    "            #t = datetime.datetime.strptime(row[3].split('.')[0], '%Y/%m/%d %H:%M:%S')\n",
    "            #t = datetime.datetime.strptime(row[10].split('.')[0], '%Y/%m/%d %H:%M:%S')\n",
    "            #gmt_t = gmt.localize(t, is_dst=None)\n",
    "            #gmt_t.astimezone (gmt)\n",
    "            res['start'].append(datetime.datetime.fromtimestamp(float(row[28])/1000,tz=pst))\n",
    "            res['end'].append(datetime.datetime.fromtimestamp(float(row[29])/1000,tz=pst))\n",
    "            res['duration'].append(float(row[30]))\n",
    "            res['size'].append(int(row[6]))\n",
    "            res['speed'].append(int(row[6]) / float(row[30]) / 1000)\n",
    "    return res\n",
    "\n",
    "tstat_dtn01 = read_tstat_from_dtn('/Users/lomax/data/tstat/dtn01.nersc.gov/2018-02-28')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following displays flows, start and times, duration, data size, and speed from first, the local TSTAT logs (from DTN's) and second from the ELK cluster. A visual matching (sorry no code yet) concludes that the missing TSTAT logs are in the local logs but not in the ELK cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-28 00:30:12.404209-08:00 146 6.670935 184\n",
      "2018-02-28 00:30:12.404227-08:00 211 6.670949 265\n",
      "2018-02-28 00:30:12.404152-08:00 205 6.671045 258\n",
      "2018-02-28 00:30:12.401261-08:00 209 6.673975 262\n",
      "2018-02-28 07:30:14.132843-08:00 192 4.105387 394\n",
      "2018-02-28 07:30:14.132850-08:00 191 4.105406 390\n",
      "2018-02-28 07:30:14.132802-08:00 194 4.105501 397\n",
      "2018-02-28 07:30:14.132563-08:00 195 4.105875 398\n",
      "2018-02-28 09:00:13.550508-08:00 230 4.64077 416\n",
      "2018-02-28 09:00:13.550547-08:00 129 4.640778 234\n",
      "2018-02-28 09:00:13.550226-08:00 232 4.641122 419\n",
      "2018-02-28 09:00:13.550528-08:00 180 4.64084 326\n",
      "2018-02-28 11:00:11.592408-08:00 92 7.638757 101\n",
      "2018-02-28 11:00:11.592266-08:00 169 7.638915 185\n",
      "2018-02-28 11:00:11.592341-08:00 301 7.638902 330\n",
      "2018-02-28 11:00:11.592046-08:00 210 7.63928 231\n",
      "2018-02-28 11:30:11.252397-08:00 216 7.531838 240\n",
      "2018-02-28 11:30:11.252106-08:00 172 7.532186 192\n",
      "2018-02-28 11:30:11.252405-08:00 169 7.531933 189\n",
      "2018-02-28 11:30:11.252381-08:00 214 7.531971 239\n",
      "2018-02-28 13:00:14.503149-08:00 235 4.937742 400\n",
      "2018-02-28 13:00:14.503164-08:00 243 4.937843 413\n",
      "2018-02-28 13:00:14.503252-08:00 52 4.937806 88\n",
      "2018-02-28 13:00:14.502802-08:00 241 4.938266 410\n",
      "2018-02-28 13:30:11.716006-08:00 192 3.917395 411\n",
      "2018-02-28 13:30:11.715909-08:00 192 3.917714 411\n",
      "2018-02-28 13:30:11.715673-08:00 195 3.918006 418\n",
      "2018-02-28 13:30:11.715959-08:00 194 3.917737 415\n",
      "2018-02-28 14:00:13.242258-08:00 194 6.44224 253\n",
      "2018-02-28 14:00:13.242459-08:00 190 6.442075 247\n",
      "2018-02-28 14:00:13.242292-08:00 191 6.442248 249\n",
      "2018-02-28 14:00:13.242095-08:00 195 6.44247 254\n",
      "2018-02-28 14:30:12.334851-08:00 174 6.492908 225\n",
      "2018-02-28 14:30:12.334828-08:00 177 6.492956 228\n",
      "2018-02-28 14:30:12.334841-08:00 173 6.492949 224\n",
      "2018-02-28 14:30:12.334553-08:00 247 6.493242 320\n",
      "2018-02-28 16:30:13.341464-08:00 188 7.06016 223\n",
      "2018-02-28 16:30:13.341094-08:00 203 7.060544 241\n",
      "2018-02-28 16:30:13.341475-08:00 105 7.060256 125\n",
      "2018-02-28 16:30:13.341318-08:00 274 7.060476 326\n",
      "2018-03-06 10:05:12.970479-08:00 181 20.904239 72\n",
      "2018-03-06 10:05:12.970500-08:00 163 20.904318 65\n",
      "2018-03-06 10:05:12.970464-08:00 214 20.904459 85\n",
      "2018-03-06 10:05:12.967252-08:00 214 20.907758 85\n",
      "2018-03-07 09:40:12.539803-08:00 192 3.813306 422\n",
      "2018-03-07 09:40:12.572443-08:00 189 3.78068 420\n",
      "2018-03-07 09:40:12.572502-08:00 196 3.780728 434\n",
      "2018-03-07 09:40:12.572477-08:00 196 3.780864 434\n",
      "2018-03-07 10:10:11.799608-08:00 268 6.10185 368\n",
      "2018-03-07 10:10:11.799589-08:00 100 6.1019 137\n",
      "2018-03-07 10:10:11.799601-08:00 268 6.101927 369\n",
      "2018-03-07 10:10:11.798910-08:00 135 6.102855 186\n",
      "2018-03-07 12:10:11.795377-08:00 265 6.269186 354\n",
      "2018-03-07 12:10:11.795280-08:00 165 6.269308 221\n",
      "2018-03-07 12:10:11.795289-08:00 153 6.269307 206\n",
      "2018-03-07 12:10:11.793824-08:00 188 6.270814 251\n"
     ]
    }
   ],
   "source": [
    "for start, end, size, duration, speed in zip(tstat_dtn01['start'],tstat_dtn01['end'],tstat_dtn01['size'],tstat_dtn01['duration'],tstat_dtn01['speed']):\n",
    "    print start,size/1024/1024, duration / 1000,int(speed * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 14:45:44-08:00 2018-02-15 14:45:54-08:00 133 0:00:10 106\n",
      "2018-02-15 14:45:44-08:00 2018-02-15 14:45:54-08:00 133 0:00:10 106\n",
      "2018-02-15 14:45:44-08:00 2018-02-15 14:45:54-08:00 250 0:00:10 200\n",
      "2018-02-15 14:45:44-08:00 2018-02-15 14:45:54-08:00 131 0:00:10 104\n",
      "2018-02-15 14:45:44-08:00 2018-02-15 14:45:54-08:00 250 0:00:10 200\n",
      "2018-02-15 14:45:44-08:00 2018-02-15 14:45:54-08:00 131 0:00:10 104\n",
      "2018-02-15 14:45:44-08:00 2018-02-15 14:45:54-08:00 294 0:00:10 235\n",
      "2018-02-15 14:45:44-08:00 2018-02-15 14:45:54-08:00 294 0:00:10 235\n",
      "2018-02-28 09:00:13-08:00 2018-02-28 09:00:18-08:00 243 0:00:05 388\n",
      "2018-02-28 09:00:13-08:00 2018-02-28 09:00:18-08:00 136 0:00:05 217\n",
      "2018-02-28 09:00:13-08:00 2018-02-28 09:00:18-08:00 189 0:00:05 302\n",
      "2018-02-28 09:00:13-08:00 2018-02-28 09:00:18-08:00 241 0:00:05 385\n",
      "2018-02-28 11:00:11-08:00 2018-02-28 11:00:19-08:00 177 0:00:08 177\n",
      "2018-02-28 11:00:11-08:00 2018-02-28 11:00:19-08:00 220 0:00:08 220\n",
      "2018-02-28 11:00:11-08:00 2018-02-28 11:00:19-08:00 315 0:00:08 315\n",
      "2018-02-28 13:00:14-08:00 2018-02-28 13:00:19-08:00 247 0:00:05 395\n",
      "2018-02-28 13:00:14-08:00 2018-02-28 13:00:19-08:00 255 0:00:05 408\n",
      "2018-02-28 13:00:14-08:00 2018-02-28 13:00:19-08:00 253 0:00:05 404\n",
      "2018-02-28 14:00:13-08:00 2018-02-28 14:00:19-08:00 199 0:00:06 265\n",
      "2018-02-28 14:00:13-08:00 2018-02-28 14:00:19-08:00 201 0:00:06 268\n",
      "2018-02-28 14:00:13-08:00 2018-02-28 14:00:19-08:00 204 0:00:06 272\n",
      "2018-02-28 14:00:13-08:00 2018-02-28 14:00:19-08:00 205 0:00:06 273\n"
     ]
    }
   ],
   "source": [
    "for start,end,size in zip(tstat['dtn01.nersc.gov']['start'],tstat['dtn01.nersc.gov']['end'],tstat['dtn01.nersc.gov']['size']):\n",
    "    print start,end,size, end - start, size * 8/(end - start).seconds"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
